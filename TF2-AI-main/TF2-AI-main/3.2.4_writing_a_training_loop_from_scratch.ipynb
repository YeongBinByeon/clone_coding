{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daf323e33b84"
   },
   "source": [
    "### 처음부터 훈련 루프 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T18:00:39.121706Z",
     "iopub.status.busy": "2021-03-12T18:00:39.121134Z",
     "iopub.status.idle": "2021-03-12T18:00:44.674694Z",
     "shell.execute_reply": "2021-03-12T18:00:44.675129Z"
    },
    "id": "ae2407ad926f"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f5a253901f8"
   },
   "source": [
    "#### 소개\n",
    "\n",
    "Keras는 기본 학습 및 평가 루프 인 fit() 및 evaluate() 합니다. \n",
    "\n",
    "이제 훈련 및 평가에 대한 매우 낮은 수준의 제어를 원한다면 처음부터 자신의 훈련 및 평가 루프를 작성해야합니다. 이것이이 가이드의 내용입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4f47351a3ec"
   },
   "source": [
    "### GradientTape 사용 : 첫 번째 종단 간 예제\n",
    "\n",
    "GradientTape 범위 내에서 모델을 호출하면 손실 값과 관련하여 학습 가능한 레이어 가중치의 기울기를 검색 할 수 있습니다. 옵티 마이저 인스턴스를 사용하면 이러한 기울기를 사용하여 이러한 변수를 업데이트 할 수 있습니다 ( model.trainable_weights 사용하여 검색 할 수 model.trainable_weights ).\n",
    "\n",
    "간단한 MNIST 모델을 고려해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T18:00:44.681217Z",
     "iopub.status.busy": "2021-03-12T18:00:44.679658Z",
     "iopub.status.idle": "2021-03-12T18:00:46.280143Z",
     "shell.execute_reply": "2021-03-12T18:00:46.280536Z"
    },
    "id": "aaa775ce7dab"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8b02a5759cf"
   },
   "source": [
    "맞춤형 훈련 루프와 함께 미니 배치 그래디언트를 사용하여 훈련 해 봅시다.\n",
    "\n",
    "먼저 옵티 마이저, 손실 함수 및 데이터 세트가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T18:00:46.287333Z",
     "iopub.status.busy": "2021-03-12T18:00:46.286725Z",
     "iopub.status.idle": "2021-03-12T18:00:46.597034Z",
     "shell.execute_reply": "2021-03-12T18:00:46.597452Z"
    },
    "id": "f2c6257b8d02"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c30285b1a2e"
   },
   "source": [
    "다음은 훈련 루프입니다.\n",
    "\n",
    "- 에포크를 반복하는 for 루프를 엽니다.\n",
    "- 각 Epoch에 대해 데이터 세트를 일괄 적으로 반복하는 for 루프를 엽니다.\n",
    "- 각 배치에 대해 GradientTape() 범위를 엽니다.\n",
    "- 이 범위 내에서 모델 (순방향 전달)을 호출하고 손실을 계산합니다.\n",
    "- 범위 밖에서 손실과 관련하여 모델 가중치의 기울기를 검색합니다.\n",
    "- 마지막으로 최적화 도구를 사용하여 기울기를 기반으로 모델의 가중치를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T18:00:46.605628Z",
     "iopub.status.busy": "2021-03-12T18:00:46.603546Z",
     "iopub.status.idle": "2021-03-12T18:00:55.260389Z",
     "shell.execute_reply": "2021-03-12T18:00:55.259854Z"
    },
    "id": "5bf4c10ceb50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.2779\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.3419\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.4533\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.1323\n",
      "Seen so far: 38464 samples\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.3922\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.2796\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.3279\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.4134\n",
      "Seen so far: 38464 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            logits = model(x_batch_train, training=True)  \n",
    "\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d600076b7be0"
   },
   "source": [
    "### 낮은 수준의 메트릭 처리\n",
    "\n",
    "이 기본 루프에 메트릭 모니터링을 추가해 보겠습니다.\n",
    "\n",
    "처음부터 작성된 교육 루프에서 기본 제공 메트릭 (또는 사용자가 작성한 사용자 지정 메트릭)을 쉽게 재사용 할 수 있습니다. 흐름은 다음과 같습니다.\n",
    "\n",
    "- 루프 시작시 메트릭 인스턴스화\n",
    "- 각 배치 후 metric.update_state() 호출\n",
    "- 메트릭의 현재 값을 표시해야하는 경우 metric.result() 호출하십시오.\n",
    "- 메트릭의 상태를 지워야 할 때 (일반적으로 epoch의 끝에서 metric.reset_states() 호출 metric.reset_states()\n",
    "\n",
    "이 지식을 사용하여 각 SparseCategoricalAccuracy 가 끝날 때 유효성 검사 데이터에 대한 SparseCategoricalAccuracy 를 계산해 SparseCategoricalAccuracy ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T18:00:55.268406Z",
     "iopub.status.busy": "2021-03-12T18:00:55.267819Z",
     "iopub.status.idle": "2021-03-12T18:00:55.302142Z",
     "shell.execute_reply": "2021-03-12T18:00:55.302534Z"
    },
    "id": "2602509b16c7"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9111a5cc87dc"
   },
   "source": [
    "다음은 훈련 및 평가 루프입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T18:00:55.311577Z",
     "iopub.status.busy": "2021-03-12T18:00:55.310930Z",
     "iopub.status.idle": "2021-03-12T18:01:06.158810Z",
     "shell.execute_reply": "2021-03-12T18:01:06.158308Z"
    },
    "id": "654e2311dbff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 89.8828\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.8284\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.6240\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.5073\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.7777\n",
      "Validation acc: 0.8416\n",
      "Time taken: 2.42s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.2146\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.0182\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.6555\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.7076\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.8688\n",
      "Validation acc: 0.8819\n",
      "Time taken: 2.42s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "940d8d9fae83"
   },
   "source": [
    "## tf.function 훈련 단계 tf.function\n",
    "\n",
    "TensorFlow 2.0의 기본 런타임은 즉시 실행 입니다. 따라서 위의 훈련 루프는 열심히 실행됩니다.\n",
    "\n",
    "이것은 디버깅에 적합하지만 그래프 컴파일에는 확실한 성능 이점이 있습니다. 계산을 정적 그래프로 설명하면 프레임 워크가 전역 성능 최적화를 적용 할 수 있습니다. 프레임 워크가 다음에 무슨 일이 일어날 지 알지 못한 채 탐욕스럽게 한 작업을 차례로 실행하도록 제한되어 있으면 불가능합니다.\n",
    "\n",
    "텐서를 입력으로 사용하는 모든 함수를 정적 그래프로 컴파일 할 수 있습니다. 다음과 같이 @tf.function 데코레이터를 추가하면됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T18:01:06.164059Z",
     "iopub.status.busy": "2021-03-12T18:01:06.163424Z",
     "iopub.status.idle": "2021-03-12T18:01:06.165173Z",
     "shell.execute_reply": "2021-03-12T18:01:06.165560Z"
    },
    "id": "fdacc2d48ade"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab61b0bf3126"
   },
   "source": [
    "평가 단계에서도 똑같이합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T18:01:06.169905Z",
     "iopub.status.busy": "2021-03-12T18:01:06.169279Z",
     "iopub.status.idle": "2021-03-12T18:01:06.171108Z",
     "shell.execute_reply": "2021-03-12T18:01:06.171442Z"
    },
    "id": "da4828fd8ef7"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    val_logits = model(x, training=False)\n",
    "    val_acc_metric.update_state(y, val_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d552377968f1"
   },
   "source": [
    "이제이 컴파일 된 학습 단계를 사용하여 학습 루프를 다시 실행 해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-12T18:01:06.178330Z",
     "iopub.status.busy": "2021-03-12T18:01:06.177755Z",
     "iopub.status.idle": "2021-03-12T18:01:08.518763Z",
     "shell.execute_reply": "2021-03-12T18:01:08.518313Z"
    },
    "id": "d69d73c94e44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.2865\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.4082\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.3934\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.3977\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.8952\n",
      "Validation acc: 0.8775\n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.2144\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.1573\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.4753\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.1794\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.9098\n",
      "Validation acc: 0.9112\n",
      "Time taken: 0.47s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8977d77a8095"
   },
   "source": [
    "훨씬 빠르죠?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "writing_a_training_loop_from_scratch.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
